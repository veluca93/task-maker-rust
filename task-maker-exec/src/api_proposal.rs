#![allow(dead_code)]

use std::sync::{Arc, Mutex};
use task_maker_dag::{
    ExecutionDAG, ExecutionGroup, ExecutionGroupUuid, ExecutionResult, ExecutionUuid, FileUuid,
};

struct FileId(usize);

// All "traits" here are both a server and a client. The client is autogenerated, and calls either
// the local or the remote object depending on the address.
// #[tmrpc::service] converts the "traits" into a client object and a server interface of some kind
// (that needs to be implemented).
// Objects that have an address can be called from anywhere; messages are routed through the server
// (but everyone connects to the server, so at most one hop can happen). These objects are assumed
// to live for as long as the client is connected to the server (or forever, for the server
// itself) -- this happens through some registration before starting the server.
// On connection to the (possibly local) server, each client/worker gets the address of the (only)
// Server instance.
//
// Note: you can't have async fns in traits, so I'm omitting the async.

enum Address<T: ?Sized> {
    Local(Arc<Mutex<T>>),
    Remote { server_id: usize, object_id: usize },
}

// #[tmrpc::service]
trait FileLoader {
    fn open_file(&self, sha256sum: &str) -> FileId;
    // Why is there a separate open_file and read_chunk?
    // - might want to read a file multiple times
    // - might want to read a file you don't know the sha of yet (i.e. streaming stdout)
    fn read_chunk(&self, file_id: &FileId) -> Vec<u8>;
}

enum ExecutionStatus {
    Start(usize),
    Done(ExecutionResult),
    Skip,
}

// #[tmrpc::service]
trait Client {
    // This is called by the server. Might also be called by the worker, but probably easier not
    // to except for streaming stdout (when we'll have it...)
    fn notify(&self, execution: ExecutionUuid, status: ExecutionStatus);
    // Server status?
    // pub fn notify_server_status(status: ServerStatus);
    // Push a file from the server to the client.
    // Doesn't return until either the file is fully received or the file is ignored by the client.
    fn push_file(&self, file: FileUuid, file_loader: Address<dyn FileLoader>, file_id: FileId);
}

// #[tmrpc::service]
trait Worker {
    fn evaluate(&self, execution: ExecutionGroup) -> ExecutionResult;
    fn kill_job(&self, execution_id: ExecutionGroupUuid);
}

// The server keeps a loop that periodically tries to dequeue a task and a worker, and send that
// task to that worker if it is an execution group, then fetch the output files from the worker. If
// the task has file dependencies that need to be satisfied by the client, it will spawn an async
// task to fetch that file from the corresponding file_loader of the client and update the DAG on
// completion.

// #[tmrpc::service]
trait Server {
    // Called by clients.
    // The server will execute the given `dag` (rather enqueue the executions), using the provided
    // file_loader to load necessary files, and will notify back the client through the provided
    // `client` address.
    // Only returns once all the information has been successfully delivered to the client.
    fn evaluate_dag(
        &mut self,
        dag: ExecutionDAG,
        file_loader: Address<dyn FileLoader>,
        client: Address<dyn Client>,
    ) -> Result<(), ()>;

    // Called by workers to register themselves as ready for execution. Workers call the function
    // again once done evaluating.
    fn register_worker(
        &mut self,
        uuid: usize,
        file_loader: Address<dyn FileLoader>,
        worker: Address<dyn Worker>,
    );
}

// No need for (Local|Remote|)Executor or WorkerManager.
// For local execution, we create as many workers as cores and start their async loop of calling
// register_worker() and waiting to be called back; they communicate with the local "server" with
// non-remote addresses. Similarly we create a local client that communicates with the "server".
// When evaluate_dag() returns on the "server", the client signals all the local workers and
// servers to terminate (not via RPC).
